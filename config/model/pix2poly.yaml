name: pix2poly
encoder:
    type: vit_small_patch${.patch_size}_${.input_size}.dino
    pretrained: True

    input_size: 224
    input_height: ${.input_size}
    input_width: ${.input_size}

    patch_size: 8
    num_patches: null
    patch_embed_dim: 384

    out_dim: 256

tokenizer: 
    num_bins: ${..encoder.input_size}
    shuffle_tokens: false
    n_vertices: 192
    max_len: null
    pad_idx: null
    generation_steps: null

fusion: patch_concat # or feature_concat

# optimization
sinkhorn_iterations: 100
label_smoothing: 0.0
vertex_loss_weight: 1.0
perm_loss_weight: 10.0

# training
batch_size: 32
start_epoch: 0
num_epochs: 100
milestone: 0
learning_rate: 4e-4
weight_decay: 1e-4


defaults:
    - lidar_encoder: pointpillars

# the following values are adapted such that the PointPillars layer can be used as a drop in for the 
# patch_embed layer of the vision transformer - vit_small_patch8_224_dino (specified in cfg.model.encoder)

lidar_encoder:
    # in theory, these are values in [m]. However, because I scale the LiDAR tile to match image coordinates, these are [m]*s, i.e. here s=224/56
    in_voxel_size: 
        x: 8 
        y: 8

    max_num_points_per_voxel: 128 # this should be something around points_per_tile (~200k) / num_voxels_per_tile (here: 784), however, I find that half of that val is enough

    max_num_voxels: # here: (224/8)**2
        train: 784 # (${.encoder.input_size} // ${.encoder.patch_size})**2
        test: 784 # (${.encoder.input_size} // ${.encoder.patch_size})**2

    out_width: 28 # ${model.encoder.input_width} // ${model.encoder.patch_size}
    out_height: 28 # ${model.encoder.input_height} // ${model.encoder.patch_size}

    out_embed_dim: ${..encoder.patch_embed_dim}

augmentations:
    - D4
    - Resize
    - ColorJitter
    - GaussNoise