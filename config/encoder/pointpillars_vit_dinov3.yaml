name: pointpillars_vit_dinov3

use_images: false
use_lidar: true

#############################
####### Point Pillars #######
#############################

in_size: 224
in_height: ${.in_size}
in_width: ${.in_size}

# in theory, these are values in [m]. However, because I scale the LiDAR tile to match image coordinates, these are [m]*s, i.e. here s=224/56, or 512/128
in_voxel_size: 
    x: ${..patch_size}
    y: ${..patch_size}
    z: 100

max_num_points_per_voxel: 256 # this should be something around points_per_tile (~200k) / num_voxels_per_tile (here: 784), however, I find that half of that val is enough

max_num_voxels: # here: (224/8)**2
    train: 196 # (${.encoder.in_size} // ${.encoder.patch_size})**2
    test: 196 # (${.encoder.in_size} // ${.encoder.patch_size})**2

##############################
##### Vision Transformer #####
##############################
type: dinov3_vits16plus
checkpoint_file: ${..dataset.out_path}/backbones/dinov3_vits16plus_pretrain_lvd1689m-4057cbaa.pth
pretrained: True

# patching
patch_size: 16
feature_map_size: 14
feature_map_height: ${.feature_map_size}
feature_map_width: ${.feature_map_size}
patch_feature_dim: 384
num_patches: 196  # int((self.cfg.experiment.encoder.in_size // self.cfg.experiment.encoder.patch_size) ** 2)

augmentations:
    - D4
    # - Resize
    - ColorJitter
    - GaussNoise
    - Normalize

# DINO v2 normalization
image_mean: [0.485, 0.456, 0.406]
image_std: [0.228, 0.224, 0.225]
image_max_pixel_value: 255.0
