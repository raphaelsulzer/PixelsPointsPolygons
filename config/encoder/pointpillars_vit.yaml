name: pointpillars_vit

use_images: false
use_lidar: true

#############################
####### Point Pillars #######
#############################

in_size: 224
in_height: ${.in_size}
in_width: ${.in_size}

# in theory, these are values in [m]. However, because I scale the LiDAR tile to match image coordinates, these are [m]*s, i.e. here s=224/56, or 512/128
in_voxel_size: 
    x: 8 # i.e. 2m
    y: 8 # i.e. 2m
    z: 100

max_num_points_per_voxel: 128 # this should be something around points_per_tile (~200k) / num_voxels_per_tile (here: 784), however, I find that half of that val is enough

max_num_voxels: # here: (224/8)**2
    train: 784 # (${.encoder.in_size} // ${.encoder.patch_size})**2
    test: 784 # (${.encoder.in_size} // ${.encoder.patch_size})**2

out_feature_width: 28 # ${model.encoder.in_width} // ${model.encoder.patch_size}
out_feature_height: 28 # ${model.encoder.in_height} // ${model.encoder.patch_size}

##############################
##### Vision Transformer #####
##############################
type: vit_small_patch${.patch_size}_${.in_size}.dino
checkpoint_file: ${..host.data_root}/checkpoints/backbones/dino_deitsmall8_pretrain.pth
pretrained: True

# patching
patch_size: 8
patch_feature_size: 28
patch_feature_height: ${.patch_feature_size}
patch_feature_width: ${.patch_feature_size}
patch_feature_dim: 384
num_patches: 784  # int((self.cfg.encoder.in_size // self.cfg.encoder.patch_size) ** 2)

out_feature_dim: ${..model.decoder.in_feature_dim}

# ViT doesn't seem to need any normalization
image_mean: [0.0, 0.0, 0.0]
image_std: [1.0, 1.0, 1.0]
image_max_pixel_value: 255.0

augmentations:
    - D4
    # - Resize
    - ColorJitter
    - GaussNoise
    - Normalize